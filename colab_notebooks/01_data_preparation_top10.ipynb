{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part 1: Enhanced Data Collection (Top 10 Cryptos)\n",
                "Dataset: BTC, ETH, BNB, SOL, XRP, ADA, AVAX, DOGE, DOT, MATIC (2 years hourly data)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CELL 1: Setup & Mount Google Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Create directories\n",
                "!mkdir -p /content/drive/MyDrive/crypto_bot/data\n",
                "!mkdir -p /content/drive/MyDrive/crypto_bot/models\n",
                "!mkdir -p /content/drive/MyDrive/crypto_bot/charts\n",
                "\n",
                "print(\"‚úÖ Google Drive mounted successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CELL 2: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q ccxt pandas numpy pandas-ta mplfinance pillow scikit-learn tqdm\n",
                "\n",
                "print(\"‚úÖ Dependencies installed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CELL 3: Fetch Top 10 Crypto Data (2 Years)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import ccxt\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from datetime import datetime, timedelta\n",
                "from tqdm.notebook import tqdm\n",
                "import time\n",
                "\n",
                "# Top 10 cryptocurrencies by market cap\n",
                "TOP_10_SYMBOLS = [\n",
                "    'BTC/USDT',   # Bitcoin\n",
                "    'ETH/USDT',   # Ethereum\n",
                "    'BNB/USDT',   # Binance Coin\n",
                "    'SOL/USDT',   # Solana\n",
                "    'XRP/USDT',   # Ripple\n",
                "    'ADA/USDT',   # Cardano\n",
                "    'AVAX/USDT',  # Avalanche\n",
                "    'DOGE/USDT',  # Dogecoin\n",
                "    'DOT/USDT',   # Polkadot\n",
                "    'MATIC/USDT'  # Polygon\n",
                "]\n",
                "\n",
                "def fetch_historical_data(symbol, timeframe='1h', days=730):\n",
                "    \"\"\"\n",
                "    Fetch 2 years of historical data from Binance\n",
                "    730 days * 24 hours = 17,520 candles per symbol\n",
                "    \"\"\"\n",
                "    exchange = ccxt.binance({\n",
                "        'enableRateLimit': True,\n",
                "        'options': {'defaultType': 'spot'}\n",
                "    })\n",
                "    \n",
                "    # Calculate start time (2 years ago)\n",
                "    end_time = datetime.now()\n",
                "    start_time = end_time - timedelta(days=days)\n",
                "    since = int(start_time.timestamp() * 1000)\n",
                "    \n",
                "    all_data = []\n",
                "    current_since = since\n",
                "    \n",
                "    print(f\"üìä Fetching {symbol}...\")\n",
                "    \n",
                "    with tqdm(total=days*24, desc=f\"{symbol}\", unit=\"candle\") as pbar:\n",
                "        while True:\n",
                "            try:\n",
                "                ohlcv = exchange.fetch_ohlcv(\n",
                "                    symbol, \n",
                "                    timeframe, \n",
                "                    since=current_since, \n",
                "                    limit=1000\n",
                "                )\n",
                "                \n",
                "                if not ohlcv:\n",
                "                    break\n",
                "                \n",
                "                all_data.extend(ohlcv)\n",
                "                pbar.update(len(ohlcv))\n",
                "                \n",
                "                # Check if we've reached current time\n",
                "                if ohlcv[-1][0] >= int(end_time.timestamp() * 1000):\n",
                "                    break\n",
                "                \n",
                "                # Move to next batch\n",
                "                current_since = ohlcv[-1][0] + 1\n",
                "                \n",
                "                # Rate limiting\n",
                "                time.sleep(exchange.rateLimit / 1000)\n",
                "                \n",
                "            except Exception as e:\n",
                "                print(f\"‚ùå Error fetching {symbol}: {e}\")\n",
                "                time.sleep(5)\n",
                "                continue\n",
                "    \n",
                "    # Convert to DataFrame\n",
                "    df = pd.DataFrame(\n",
                "        all_data, \n",
                "        columns=['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
                "    )\n",
                "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
                "    df['symbol'] = symbol.replace('/USDT', '')\n",
                "    \n",
                "    # Remove duplicates\n",
                "    df = df.drop_duplicates(subset='timestamp').reset_index(drop=True)\n",
                "    \n",
                "    print(f\"‚úÖ {symbol}: {len(df)} candles ({df['timestamp'].min()} to {df['timestamp'].max()})\")\n",
                "    \n",
                "    return df\n",
                "\n",
                "# Fetch data for all top 10 cryptocurrencies\n",
                "print(\"üöÄ Fetching 2 years of data for Top 10 cryptocurrencies...\")\n",
                "print(f\"   Expected: ~17,520 candles per symbol\")\n",
                "print(f\"   Total: ~175,200 data points\\n\")\n",
                "\n",
                "all_crypto_data = {}\n",
                "\n",
                "for symbol in TOP_10_SYMBOLS:\n",
                "    try:\n",
                "        df = fetch_historical_data(symbol, timeframe='1h', days=730)\n",
                "        all_crypto_data[symbol] = df\n",
                "        \n",
                "        # Save individual CSV\n",
                "        crypto_name = symbol.replace('/USDT', '').lower()\n",
                "        df.to_csv(f'/content/drive/MyDrive/crypto_bot/data/{crypto_name}_2y_raw.csv', index=False)\n",
                "        \n",
                "        # Small delay between symbols\n",
                "        time.sleep(2)\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Failed to fetch {symbol}: {e}\")\n",
                "        continue\n",
                "\n",
                "print(f\"\\n‚úÖ Successfully fetched {len(all_crypto_data)} cryptocurrencies\")\n",
                "print(f\"   Total data points: {sum(len(df) for df in all_crypto_data.values()):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CELL 4: Add Technical Indicators (All Symbols)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas_ta as ta\n",
                "\n",
                "def add_technical_indicators(df):\n",
                "    \"\"\"\n",
                "    Add comprehensive technical indicators for ML features\n",
                "    \"\"\"\n",
                "    df = df.copy()\n",
                "    \n",
                "    # Momentum Indicators\n",
                "    df['rsi'] = ta.rsi(df['close'], length=14)\n",
                "    df['rsi_ma'] = df['rsi'].rolling(14).mean()\n",
                "    df['rsi_std'] = df['rsi'].rolling(14).std()\n",
                "    \n",
                "    # MACD\n",
                "    macd = ta.macd(df['close'], fast=12, slow=26, signal=9)\n",
                "    df['macd'] = macd['MACD_12_26_9']\n",
                "    df['macd_signal'] = macd['MACDs_12_26_9']\n",
                "    df['macd_hist'] = macd['MACDh_12_26_9']\n",
                "    \n",
                "    # Moving Averages\n",
                "    df['sma_7'] = ta.sma(df['close'], length=7)\n",
                "    df['sma_20'] = ta.sma(df['close'], length=20)\n",
                "    df['sma_50'] = ta.sma(df['close'], length=50)\n",
                "    df['sma_100'] = ta.sma(df['close'], length=100)\n",
                "    df['sma_200'] = ta.sma(df['close'], length=200)\n",
                "    \n",
                "    df['ema_12'] = ta.ema(df['close'], length=12)\n",
                "    df['ema_26'] = ta.ema(df['close'], length=26)\n",
                "    df['ema_50'] = ta.ema(df['close'], length=50)\n",
                "    \n",
                "    # Bollinger Bands\n",
                "    bbands = ta.bbands(df['close'], length=20, std=2)\n",
                "    df['bb_upper'] = bbands['BBU_20_2.0']\n",
                "    df['bb_middle'] = bbands['BBM_20_2.0']\n",
                "    df['bb_lower'] = bbands['BBL_20_2.0']\n",
                "    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']\n",
                "    df['bb_percent'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])\n",
                "    \n",
                "    # Volume indicators\n",
                "    df['volume_sma'] = ta.sma(df['volume'], length=20)\n",
                "    df['volume_ratio'] = df['volume'] / df['volume_sma']\n",
                "    df['volume_std'] = df['volume'].rolling(20).std()\n",
                "    \n",
                "    # ATR (volatility)\n",
                "    df['atr'] = ta.atr(df['high'], df['low'], df['close'], length=14)\n",
                "    df['atr_percent'] = (df['atr'] / df['close']) * 100\n",
                "    \n",
                "    # Stochastic\n",
                "    stoch = ta.stoch(df['high'], df['low'], df['close'], k=14, d=3)\n",
                "    df['stoch_k'] = stoch['STOCHk_14_3_3']\n",
                "    df['stoch_d'] = stoch['STOCHd_14_3_3']\n",
                "    \n",
                "    # ADX (trend strength)\n",
                "    adx = ta.adx(df['high'], df['low'], df['close'], length=14)\n",
                "    df['adx'] = adx['ADX_14']\n",
                "    df['di_plus'] = adx['DMP_14']\n",
                "    df['di_minus'] = adx['DMN_14']\n",
                "    \n",
                "    # Price momentum\n",
                "    df['momentum'] = df['close'].pct_change(periods=10) * 100\n",
                "    df['rate_of_change'] = ta.roc(df['close'], length=10)\n",
                "    \n",
                "    # Price position relative to highs/lows\n",
                "    df['high_20'] = df['high'].rolling(20).max()\n",
                "    df['low_20'] = df['low'].rolling(20).min()\n",
                "    df['price_position'] = (df['close'] - df['low_20']) / (df['high_20'] - df['low_20'])\n",
                "    \n",
                "    # Williams %R\n",
                "    df['williams_r'] = ta.willr(df['high'], df['low'], df['close'], length=14)\n",
                "    \n",
                "    # CCI (Commodity Channel Index)\n",
                "    df['cci'] = ta.cci(df['high'], df['low'], df['close'], length=20)\n",
                "    \n",
                "    # Returns\n",
                "    df['returns_1h'] = df['close'].pct_change(1)\n",
                "    df['returns_24h'] = df['close'].pct_change(24)\n",
                "    df['returns_7d'] = df['close'].pct_change(168)  # 7 days * 24 hours\n",
                "    \n",
                "    # Volatility\n",
                "    df['volatility_24h'] = df['returns_1h'].rolling(24).std()\n",
                "    df['volatility_7d'] = df['returns_1h'].rolling(168).std()\n",
                "    \n",
                "    # Drop NaN rows\n",
                "    df = df.dropna().reset_index(drop=True)\n",
                "    \n",
                "    return df\n",
                "\n",
                "print(\"üîÑ Adding technical indicators to all symbols...\\n\")\n",
                "\n",
                "processed_data = {}\n",
                "\n",
                "for symbol, df in all_crypto_data.items():\n",
                "    print(f\"Processing {symbol}...\")\n",
                "    processed_df = add_technical_indicators(df)\n",
                "    processed_data[symbol] = processed_df\n",
                "    \n",
                "    # Save processed data\n",
                "    crypto_name = symbol.replace('/USDT', '').lower()\n",
                "    processed_df.to_csv(\n",
                "        f'/content/drive/MyDrive/crypto_bot/data/{crypto_name}_2y_processed.csv', \n",
                "        index=False\n",
                "    )\n",
                "    \n",
                "    print(f\"‚úÖ {symbol}: {len(processed_df.columns)} features, {len(processed_df)} rows\")\n",
                "\n",
                "# Combine all into one master dataset\n",
                "master_df = pd.concat(processed_data.values(), ignore_index=True)\n",
                "master_df.to_csv('/content/drive/MyDrive/crypto_bot/data/master_top10_2y.csv', index=False)\n",
                "\n",
                "print(f\"\\n‚úÖ Master dataset created:\")\n",
                "print(f\"   Total rows: {len(master_df):,}\")\n",
                "print(f\"   Total features: {len(master_df.columns)}\")\n",
                "print(f\"   Symbols: {master_df['symbol'].nunique()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CELL 5: Generate Chart Images (50,000 Images)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import mplfinance as mpf\n",
                "from PIL import Image\n",
                "import matplotlib\n",
                "matplotlib.use('Agg')  # Non-interactive backend for faster generation\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "def generate_chart_images_batch(df, symbol, num_images=5000, lookback=100):\n",
                "    \"\"\"\n",
                "    Generate candlestick chart images with labels\n",
                "    \n",
                "    Labels based on next 24h return:\n",
                "    - 0: Bearish (< -2%)\n",
                "    - 1: Neutral (-2% to +2%)\n",
                "    - 2: Bullish (> +2%)\n",
                "    \"\"\"\n",
                "    \n",
                "    images = []\n",
                "    labels = []\n",
                "    metadata = []\n",
                "    \n",
                "    # Ensure we have enough data\n",
                "    max_start = len(df) - lookback - 24\n",
                "    if max_start < num_images:\n",
                "        num_images = max_start\n",
                "    \n",
                "    # Random sampling for diversity\n",
                "    indices = np.random.choice(\n",
                "        range(lookback, max_start), \n",
                "        size=num_images, \n",
                "        replace=False\n",
                "    )\n",
                "    \n",
                "    print(f\"üìä Generating {num_images} images for {symbol}...\")\n",
                "    \n",
                "    for idx in tqdm(indices, desc=symbol):\n",
                "        try:\n",
                "            # Get chart window\n",
                "            chart_data = df.iloc[idx-lookback:idx].copy()\n",
                "            chart_data = chart_data[['open', 'high', 'low', 'close', 'volume']].copy()\n",
                "            chart_data.index = pd.DatetimeIndex(df.iloc[idx-lookback:idx]['timestamp'])\n",
                "            \n",
                "            # Calculate future return\n",
                "            current_price = df.iloc[idx]['close']\n",
                "            future_price = df.iloc[idx + 24]['close']\n",
                "            future_return = (future_price - current_price) / current_price * 100\n",
                "            \n",
                "            # Assign label (adjusted thresholds for more balance)\n",
                "            if future_return < -2:\n",
                "                label = 0  # Bearish\n",
                "            elif future_return > 2:\n",
                "                label = 2  # Bullish\n",
                "            else:\n",
                "                label = 1  # Neutral\n",
                "            \n",
                "            # Generate chart\n",
                "            fig, axes = mpf.plot(\n",
                "                chart_data,\n",
                "                type='candle',\n",
                "                style='charles',\n",
                "                volume=True,\n",
                "                mav=(20, 50),\n",
                "                figsize=(6, 4),\n",
                "                returnfig=True,\n",
                "                tight_layout=True,\n",
                "                warn_too_much_data=10000\n",
                "            )\n",
                "            \n",
                "            # Convert to array\n",
                "            fig.canvas.draw()\n",
                "            img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
                "            img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
                "            plt.close(fig)\n",
                "            \n",
                "            # Resize to 224x224\n",
                "            img = Image.fromarray(img).resize((224, 224), Image.LANCZOS)\n",
                "            img_array = np.array(img)\n",
                "            \n",
                "            images.append(img_array)\n",
                "            labels.append(label)\n",
                "            metadata.append({\n",
                "                'symbol': symbol,\n",
                "                'index': int(idx),\n",
                "                'timestamp': str(df.iloc[idx]['timestamp']),\n",
                "                'current_price': float(current_price),\n",
                "                'future_return': float(future_return),\n",
                "                'label': int(label)\n",
                "            })\n",
                "            \n",
                "        except Exception as e:\n",
                "            # Skip problematic charts\n",
                "            continue\n",
                "    \n",
                "    images = np.array(images)\n",
                "    labels = np.array(labels)\n",
                "    \n",
                "    print(f\"‚úÖ Generated {len(images)} images\")\n",
                "    print(f\"   Distribution: Bearish={sum(labels==0)}, Neutral={sum(labels==1)}, Bullish={sum(labels==2)}\")\n",
                "    \n",
                "    return images, labels, metadata\n",
                "\n",
                "# Generate 5,000 images per symbol (50,000 total)\n",
                "print(\"\\nüé® Generating 50,000 chart images across all symbols...\")\n",
                "print(\"   This will take 30-45 minutes...\\n\")\n",
                "\n",
                "all_images = []\n",
                "all_labels = []\n",
                "all_metadata = []\n",
                "\n",
                "for symbol, df in processed_data.items():\n",
                "    try:\n",
                "        images, labels, metadata = generate_chart_images_batch(\n",
                "            df, \n",
                "            symbol, \n",
                "            num_images=5000, \n",
                "            lookback=100\n",
                "        )\n",
                "        \n",
                "        all_images.append(images)\n",
                "        all_labels.append(labels)\n",
                "        all_metadata.extend(metadata)\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error generating charts for {symbol}: {e}\")\n",
                "        continue\n",
                "\n",
                "# Combine all images\n",
                "all_images = np.concatenate(all_images)\n",
                "all_labels = np.concatenate(all_labels)\n",
                "\n",
                "print(f\"\\n‚úÖ Total chart images generated: {len(all_images):,}\")\n",
                "print(f\"   Shape: {all_images.shape}\")\n",
                "print(f\"   File size: ~{all_images.nbytes / (1024**3):.2f} GB\")\n",
                "print(f\"\\nüìä Label Distribution:\")\n",
                "print(f\"   Bearish (0): {sum(all_labels==0):,} ({sum(all_labels==0)/len(all_labels)*100:.1f}%)\")\n",
                "print(f\"   Neutral (1): {sum(all_labels==1):,} ({sum(all_labels==1)/len(all_labels)*100:.1f}%)\")\n",
                "print(f\"   Bullish (2): {sum(all_labels==2):,} ({sum(all_labels==2)/len(all_labels)*100:.1f}%)\")\n",
                "\n",
                "# Save to Drive\n",
                "np.save('/content/drive/MyDrive/crypto_bot/data/chart_images_top10.npy', all_images)\n",
                "np.save('/content/drive/MyDrive/crypto_bot/data/chart_labels_top10.npy', all_labels)\n",
                "\n",
                "# Save metadata\n",
                "import json\n",
                "with open('/content/drive/MyDrive/crypto_bot/data/chart_metadata_top10.json', 'w') as f:\n",
                "    json.dump(all_metadata, f, indent=2)\n",
                "\n",
                "print(\"\\n‚úÖ All images saved to Google Drive!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CELL 6: Data Statistics & Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"üìä DATA COLLECTION SUMMARY - TOP 10 CRYPTOCURRENCIES\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "print(f\"\\nüíæ OHLCV Data:\")\n",
                "print(f\"   Symbols: {len(processed_data)}\")\n",
                "print(f\"   Total rows: {len(master_df):,}\")\n",
                "print(f\"   Features: {len(master_df.columns)}\")\n",
                "print(f\"   Date range: {master_df['timestamp'].min()} to {master_df['timestamp'].max()}\")\n",
                "\n",
                "print(f\"\\nüé® Chart Images:\")\n",
                "print(f\"   Total images: {len(all_images):,}\")\n",
                "print(f\"   Image shape: {all_images.shape[1:]}\")\n",
                "print(f\"   Dataset size: {all_images.nbytes / (1024**3):.2f} GB\")\n",
                "\n",
                "print(f\"\\nüìà Per-Symbol Breakdown:\")\n",
                "for symbol in TOP_10_SYMBOLS:\n",
                "    crypto_name = symbol.replace('/USDT', '')\n",
                "    symbol_data = master_df[master_df['symbol'] == crypto_name]\n",
                "    symbol_images = sum(1 for m in all_metadata if m['symbol'] == symbol)\n",
                "    print(f\"   {crypto_name:6s}: {len(symbol_data):,} rows, {symbol_images:,} images\")\n",
                "\n",
                "print(\"\\n‚úÖ DATA PREPARATION COMPLETE!\")\n",
                "print(\"\\nFiles saved to Google Drive:\")\n",
                "print(\"   ‚Ä¢ Individual CSVs: data/{symbol}_2y_processed.csv\")\n",
                "print(\"   ‚Ä¢ Master dataset: data/master_top10_2y.csv\")\n",
                "print(\"   ‚Ä¢ Chart images: data/chart_images_top10.npy\")\n",
                "print(\"   ‚Ä¢ Chart labels: data/chart_labels_top10.npy\")\n",
                "print(\"   ‚Ä¢ Metadata: data/chart_metadata_top10.json\")\n",
                "\n",
                "print(\"\\nüöÄ Next: Run notebook 02_train_lstm_top10.ipynb\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}